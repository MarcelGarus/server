import make.mar

struct Token { str: String, kind: TokenKind }
enum TokenKind {
  default,
  comment,     | comments
  declaration, | declaration keywords such as struct, enum, fun, fn
  control,     | control keywords such as if, switch, match, comptime
  type,        | types such as Foo, Bar
  name,        | names such as foo, bar
  literal,     | literal values such as strings or integers
}
fun ==(a: TokenKind, b: TokenKind): Bool {
  enum_to_byte(a) == enum_to_byte(b)
}
fun enum_to_byte[T](instance: T): Byte {
  enum_to_byte_impl(instance.&, size_of[T]())
}
fun enum_to_byte_impl[T](instance: &T, size: Int): Byte asm {
  moveib a 8 add a sp load a a  | instance
  moveib b 16 add b sp load b b | size
  add a b moveib c 1 sub a c loadb c a | {instance + size - 1}.*
  load a sp | return address
  store a c ret
}

fun is_supported_language(lang: String): Bool {
  lang == "text" or
    lang == "bash" or
    lang == "c" or
    lang == "candy" or
    lang == "dart" or
    lang == "html" or
    lang == "json" or
    lang == "lisp" or
    lang == "markdown" or
    lang == "mar" or
    lang == "mehl" or
    lang == "path" or
    lang == "plum" or
    lang == "python" or
    lang == "rust" or
    lang == "scopes" or
    lang == "soil" or
    lang == "zig"
}

fun tokenize(language: String, code: String): Slice[Token] {
  tokenize_unmerged(language, code).merge_some_tokens()
}
fun merge_some_tokens(tokens: Slice[Token]): Slice[Token] {
  var out = list[Token]()
  for token in tokens do {
    if out.is_empty() then { out.&.push(token)  continue } | first token

    if out.last().kind == token.kind or token.str.trim().is_empty()
    then out.&.last_ref().str = "{out.last().str}{token.str}"
    else out.&.push(token)
  }
  out.to_slice()
}
fun tokenize_unmerged(language: String, code: String): Slice[Token] {
  if language == "text" then return tokenize_text(code)
  if language == "bash" then return tokenize_bash(code)
  if language == "c" then return tokenize_c(code)
  if language == "candy" then return tokenize_candy(code)
  if language == "dart" then return tokenize_dart(code)
  if language == "html" then return tokenize_html(code)
  if language == "json" then return tokenize_json(code)
  if language == "lisp" then return tokenize_lisp(code)
  if language == "markdown" then return tokenize_markdown(code)
  if language == "mar" then return tokenize_martinaise(code)
  if language == "mehl" then return tokenize_mehl(code)
  if language == "path" then return tokenize_path(code)
  if language == "plum" then return tokenize_plum(code)
  if language == "python" then return tokenize_python(code)
  if language == "rust" then return tokenize_rust(code)
  if language == "scopes" then return tokenize_scopes(code)
  if language == "soil" then return tokenize_soil(code)
  if language == "zig" then return tokenize_zig(code)

  panic("I don't know how to tokenize {language}. Code:\n{code}")
}

fun tokenize_text(text: String): Slice[Token] {
  filled_slice[Token](1, Token { str = text, kind = TokenKind.default })
}

| Crudely tokenizes a programming languages. Uses comment_start as an indicator
| for a line comment, double quotes for strings, and identifier_chars to group
| adjacent chars into identifiers.
fun tokenize_programming_language(
  input: String, comment_start: String, identifier_chars: String
): Slice[String] {
  var out = list[String]()
  var cursor = 0
  loop {
    if cursor >= input.len then break
    var rest = input.without_first(cursor)

    if rest.starts_with(comment_start) then {
      var end = cursor
      loop {
        if {input.get_maybe(end) or break} == newline then break
        end = end + 1
      }
      out.&.push(input.substr(cursor..end))
      cursor = end
      continue
    }

    if rest.get(0) == #" then {
      var end = cursor
      loop {
        end = end + 1
        if {input.get_maybe(end) or break} == #" then {
          end = end + 1
          break
        }
      }
      out.&.push(input.substr(cursor..end))
      cursor = end
      continue
    }

    if identifier_chars.iter().&.contains(rest.get(0)) then {
      var end = cursor
      loop {
        if not(identifier_chars.iter().&.contains(input.get_maybe(end) or break)) then break
        end = end + 1
      }
      out.&.push(input.substr(cursor..end))
      cursor = end
      continue
    }

    out.&.push(rest.substr(0..1))
    cursor = cursor + 1
  }
  out.to_slice()
}

fun tokenize_bash(text: String): Slice[Token] {
  var out = list[Token]()
  for line in text.lines() do {
    if line.chars().iter().&.contains(#$) then {
      var parts = line.split("$")
      var prompt = parts.without_last().join("$")
      var command = parts.last()
      out.&.push(Token { str = prompt, kind = TokenKind.declaration })
      out.&.push(Token { str = "$", kind = TokenKind.declaration })
      out.&.push(Token { str = command, kind = TokenKind.control })
    } else {
      out.&.push(Token { str = line, kind = TokenKind.default })
    }
    out.&.push(Token { str = "\n", kind = TokenKind.default })
  }
  out.to_slice()
}

fun tokenize_c(code: String): Slice[Token] {
  var declaration_keywords = list("struct", "union", "enum")
  var control_keywords = list("if", "else", "switch", "case", "default",
    "while", "for", "break", "continue", "return")
  var types = list("int", "float", "double", "long", "perf_event_attr")

  var out = list[Token]()
  for token in code.tokenize_programming_language(
    "//", "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789")
  do {
    out.&.push(Token {
      str = token,
      kind =
        if token.starts_with("//") then TokenKind.comment
        else if declaration_keywords.iter().&.contains(token) then TokenKind.declaration
        else if control_keywords.iter().&.contains(token) then TokenKind.control
        else if types.iter().&.contains(token) then TokenKind.type
        else if token.chars().get(0) == #" then TokenKind.literal
        else if token.chars().get(0).is_lower() then TokenKind.name
        else if token.chars().get(0).is_digit() then TokenKind.literal
        else TokenKind.default
    })
  }
  out.to_slice()
}

fun tokenize_candy(code: String): Slice[Token] {
  var out = list[Token]()
  for token in code.tokenize_programming_language(
    "#", "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789")
  do {
    out.&.push(Token {
      str = token,
      kind =
        if token.starts_with("#") then TokenKind.comment
        else if token == "needs" then TokenKind.control
        else if token.chars().get(0) == #" then TokenKind.literal
        else if token.chars().get(0).is_lower() then TokenKind.name
        else if token.chars().get(0).is_digit() then TokenKind.literal
        else TokenKind.default
    })
  }
  out.to_slice()
}

fun tokenize_dart(code: String): Slice[Token] {
  var declaration_keywords = list("final", "var", "class", "mixin", "extends")
  var control_keywords = list("if", "else", "switch", "case", "default",
    "loop", "for", "in", "do", "while", "break", "continue", "return")

  var out = list[Token]()
  for token in code.tokenize_programming_language(
    "//", "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789")
  do {
    out.&.push(Token {
      str = token,
      kind =
        if token.starts_with("//") then TokenKind.comment
        else if declaration_keywords.iter().&.contains(token) then TokenKind.declaration
        else if control_keywords.iter().&.contains(token) then TokenKind.control
        else if token.chars().get(0) == #" then TokenKind.literal
        else if token.chars().get(0).is_upper() then TokenKind.type
        else if token.chars().get(0).is_lower() then TokenKind.name
        else if token.chars().get(0).is_digit() then TokenKind.literal
        else TokenKind.default
    })
  }
  out.to_slice()
}

fun tokenize_html(code: String): Slice[Token] {
  var out = list[Token]()
  var in_tag = false
  for char in code do {
    if char == #< then in_tag = true
    out.&.push(Token {
      str = char.format(),
      kind = if in_tag then TokenKind.control else TokenKind.default,
    })
    if char == #> then in_tag = false
  }
  out.to_slice()
}

fun tokenize_json(json: String): Slice[Token] {
  var out = list[Token]()
  var cursor = 0
  loop {
    if cursor >= json.len then break

    if json.get(cursor) == #" then {
      var end = cursor + 1
      loop {
        if {json.get_maybe(end) or break} == #" then break
        end = end + 1
      }
      out.&.push(Token {
        str = json.substr(cursor..{end + 1}), kind = TokenKind.literal
      })
      cursor = end + 1
      continue
    }

    out.&.push(Token {
      str = json.get(cursor).format(),
      kind = if digit_chars.contains(json.get(cursor))
        then TokenKind.literal
        else TokenKind.default
    })
    cursor = cursor + 1
  }
  out.to_slice()
}

fun tokenize_lisp(text: String): Slice[Token] {
  var out = list[Token]()
  for char in text do
    out.&.push(Token {
      str = char.format(),
      kind = if char == #( or char == #) then TokenKind.default else TokenKind.name,
    })
  out.to_slice()
}

fun tokenize_markdown(text: String): Slice[Token] {
  var out = list[Token]()
  for token in text.tokenize_programming_language("#", "") do {
    out.&.push(Token {
      str = token,
      kind =
        if token.starts_with("#") then TokenKind.comment
        else if token == ">" or token == "-" then TokenKind.declaration
        else if token == "*" or token == "_" then TokenKind.control
        else if token == "[" or token == "]" or token == "(" or token == ")" or token == "!" then TokenKind.literal
        else TokenKind.default
    })
  }
  out.to_slice()
}

fun tokenize_martinaise(code: String): Slice[Token] {
  var declaration_keywords = list("fun", "var", "struct", "enum", "opaque")
  var control_keywords = list("if", "then", "else", "switch", "case", "default",
    "orelse", "and", "or", "loop", "for", "in", "do", "break", "continue",
    "return") + list("is")
  var operator_chars = "%!~@^\\/`.&*+$-<>="

  var out = list[Token]()
  for token in code.tokenize_programming_language(
    "|", "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_")
  do {
    out.&.push(Token {
      str = token,
      kind =
        if token.starts_with("|") then TokenKind.comment
        else if declaration_keywords.iter().&.contains(token) then TokenKind.declaration
        else if control_keywords.iter().&.contains(token) then TokenKind.control
        else if token.chars().get(0) == #" then TokenKind.literal
        else if token.chars().get(0).is_upper() then TokenKind.type
        else if token.chars().get(0).is_lower() then TokenKind.name
        else if token.chars().get(0).is_digit() then TokenKind.literal
        else TokenKind.default
    })
  }
  out.to_slice()
}

fun tokenize_mehl(code: String): Slice[Token] {
  var out = list[Token]()
  for token in code.tokenize_programming_language(
    "#", "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789:=>.")
  do {
    out.&.push(Token {
      str = token,
      kind =
        if token.starts_with("#") then TokenKind.comment
        else if token == "=>" then TokenKind.declaration
        else if token == "." then TokenKind.control
        else if token.starts_with(":") then TokenKind.literal
        else if token.chars().get(0) == #" then TokenKind.literal
        else if token.chars().get(0).is_digit() then TokenKind.literal
        else if token.chars().get(0).is_lower() then TokenKind.name
        else TokenKind.default
    })
  }
  out.to_slice()
}

fun tokenize_path(path: String): Slice[Token] {
  filled_slice[Token](1, Token { str = path, kind = TokenKind.default })
}

fun tokenize_plum(code: String): Slice[Token] {
  var out = list[Token]()
  for token in code.tokenize_programming_language(
    "#", "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_")
  do {
    out.&.push(Token {
      str = token,
      kind =
        if token.starts_with("#") then TokenKind.comment
        else if token == "&" then TokenKind.declaration
        else if token == "|" then TokenKind.declaration
        else if token == "%" then TokenKind.control
        else if token.chars().get(0).is_letter() or token.chars().get(0) == #_ then TokenKind.name
        else if token.chars().get(0).is_digit() then TokenKind.literal
        else TokenKind.default
    })
  }
  out.to_slice()
}

fun tokenize_python(code: String): Slice[Token] {
  var declaration_keywords = list("from", "import", "as", "class")
  var control_keywords = list("if", "else", "while", "for", "in", "break",
    "continue", "return", "and", "or")

  var out = list[Token]()
  for token in code.tokenize_programming_language(
    "#", "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_")
  do {
    out.&.push(Token {
      str = token,
      kind =
        if token.starts_with("#") then TokenKind.comment
        else if declaration_keywords.iter().&.contains(token) then TokenKind.declaration
        else if control_keywords.iter().&.contains(token) then TokenKind.control
        else if token.chars().get(0).is_letter() or token.chars().get(0) == #_ then TokenKind.name
        else if token.chars().get(0).is_digit() then TokenKind.literal
        else TokenKind.default
    })
  }
  out.to_slice()
}

fun tokenize_rust(code: String): Slice[Token] {
  var declaration_keywords = list("fn", "let", "mut", "const", "struct", "enum",
    "pub", "impl", "trait", "self")
  var control_keywords = list("if", "else", "match", "loop", "while", "for",
    "in", "break", "continue", "return")
  var types = list("bool", "usize", "u8", "u16", "str")
  var literals = list("true", "false", "Some", "None")

  var out = list[Token]()
  for token in code.tokenize_programming_language(
    "//", "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_")
  do {
    out.&.push(Token {
      str = token,
      kind =
        if token.starts_with("//") then TokenKind.comment
        else if declaration_keywords.iter().&.contains(token) then TokenKind.declaration
        else if control_keywords.iter().&.contains(token) then TokenKind.control
        else if types.iter().&.contains(token) then TokenKind.type
        else if literals.iter().&.contains(token) then TokenKind.literal
        else if token.chars().get(0) == #" then TokenKind.literal
        else if token.chars().get(0).is_upper() then TokenKind.type
        else if token.chars().get(0).is_lower() then TokenKind.name
        else if token.chars().get(0).is_digit() then TokenKind.literal
        else TokenKind.default
    })
  }
  out.to_slice()
}

fun tokenize_scopes(scopes: String): Slice[Token] {
  var out = list[Token]()
  for token
  in
    scopes.tokenize_programming_language(
      "#", "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
    )
  do out.&.push(Token { str = token, kind = TokenKind.name })
  out.to_slice()
}

fun tokenize_soil(soil: String): Slice[Token] {
  var instructions = list("nop", "panic", "trystart", "tryend")
    + list("move", "movei", "moveib", "load", "loadb", "store", "storeb", "push",
      "pop", "jump")
    + list("cjump", "call", "ret", "syscall")
    + list("cmp", "isequal", "isless", "isgreater", "islessequal",
      "isgreaterequal", "isnotequal", "fcmp", "fisequal", "fisless",
      "fisgreater", "fislessequal", "fisgreaterequal", "fisnotequal",
      "inttofloat", "floattoint")
    + list("add", "sub", "mul", "div", "rem", "fadd", "fsub", "fmul", "fdiv",
      "and", "or", "xor", "not")
  var registers = list("sp", "st", "a", "b", "c", "d", "e", "f")

  var out = list[Token]()
  for token in soil.tokenize_programming_language(
    "|", "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789")
  do {
    out.&.push(Token {
      str = token,
      kind =
        if token.starts_with("|") then TokenKind.comment
        else if instructions.iter().&.contains(token) then TokenKind.control
        else if registers.iter().&.contains(token) then TokenKind.name
        else if token.chars().get(0).is_digit() then TokenKind.literal
        else if token.chars().get(0).is_letter() then TokenKind.name
        else TokenKind.default
    })
  }
  out.to_slice()
}

fun tokenize_zig(code: String): Slice[Token] {
  var declaration_keywords = list("pub", "fn", "comptime", "const", "var",
    "struct", "enum", "union", "unreachable")
  var control_keywords = list("if", "else", "inline", "while", "for", "break",
    "continue", "return", "switch", "try", "orelse", "catch")
  var types = list("void", "type", "anytype", "bool", "usize", "u8", "u64", "i64")
  var literals = list("true", "false")

  var out = list[Token]()
  for token in code.tokenize_programming_language(
    "//", "@abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_")
  do {
    out.&.push(Token {
      str = token,
      kind =
        if token.starts_with("//") then TokenKind.comment
        else if declaration_keywords.iter().&.contains(token) then TokenKind.declaration
        else if control_keywords.iter().&.contains(token) then TokenKind.control
        else if types.iter().&.contains(token) then TokenKind.type
        else if literals.iter().&.contains(token) then TokenKind.literal
        else if token.chars().get(0) == #" then TokenKind.literal
        else if token.chars().get(0).is_digit() then TokenKind.literal
        else if token.chars().get(0).is_upper() then TokenKind.type
        else if token.chars().get(0).is_lower() then TokenKind.name
        else if token.chars().get(0) == #@ then {
          if token.len == 1 then TokenKind.default
          else if token.chars().get(1).is_upper() then TokenKind.type
          else if token.chars().get(1).is_lower() then TokenKind.name
          else TokenKind.default
        }
        else TokenKind.default
    })
  }
  out.to_slice()
}
